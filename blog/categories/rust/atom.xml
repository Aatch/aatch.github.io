<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: rust | Adventures in the Absurd]]></title>
  <link href="http://aatch.github.io/blog/categories/rust/atom.xml" rel="self"/>
  <link href="http://aatch.github.io/"/>
  <updated>2013-10-29T17:28:55+13:00</updated>
  <id>http://aatch.github.io/</id>
  <author>
    <name><![CDATA[James Miller]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[A Rustic Linker Model]]></title>
    <link href="http://aatch.github.io/blog/2013/10/29/a-rustic-linker-model/"/>
    <updated>2013-10-29T13:15:00+13:00</updated>
    <id>http://aatch.github.io/blog/2013/10/29/a-rustic-linker-model</id>
    <content type="html"><![CDATA[<p>This is a kinda-sorta proposal for a linking model for the rust compiler to use, allowing us to
separate out from system-specific semantics and avoid being tied too closely to a system linker.
The goals are to maintain most compatibility with system tools (linkers in particular), allow
common cases to behave identically on all supported platforms and maintain an escape hatch to allow
complex and/or esoteric cases to still work without too much hackery or relying on undocumented
behaviour.</p>

<!-- More -->


<h2>What is linking?</h2>

<p>If you are already familiar with the concept of linking, feel free to skip this section, otherwise
here is a quick rundown of what linking is.</p>

<p>If you come from a primarily dynamic-language background (Python, Ruby, etc) then the concept of
linking is likely unfamiliar to you. However, it is an important step in the process of turning
your code into a library that can be shared or an executable that actually does something. Briefly,
linking is the process of taking compiled code and &ldquo;linking&rdquo; it with other compiled code to allow
that code to call out to other libraries.</p>

<p>When you compile a Rust library, one step produces a temporary file called an <em>Object File</em>. This
is a file in a specific format depending on what platform you&rsquo;re on. On Linux, this file is in the
ELF format. Now, this file only contains your code, so if you called any functions or methods from
other places you have references to those in your code, they are called <em>Unresolved Symbols</em> and
until they are resolved your code won&rsquo;t work.</p>

<p>The process of linking mostly involves finding out where those unresolved symbols live and, well,
resolving them. The linker will look in the libraries it has been told to for those symbols and
makes a note of where it found the symbol. While the exact behaviour depends on what type of file
you want to make, the process of making that file is effectively the same for both executables and
libraries.</p>

<p>If an unresolved symbol is found in a static library, then the code associated with that library is
copied into the new file and the symbol is resolved to point there. If the symbol is found in
a dynamic library, then the name of that file is added to a special list in the new file. When the
file is loaded (either in order to run it, or because another file needs it), this list is used to
load any required files and dynamically resolve any symbols needed.</p>

<h2>Platform Compatibility</h2>

<p>This, to me, is probably the most important point. While there is a difference between being
compatible and being user-friendly, being incompatible with platform tools could severely limit the
situations that Rust would otherwise work in. This means using platform-specific formats and making
sure that any Rust-specific functionality can be implemented across the majority of platforms.</p>

<p>Fortunately, this goal is actually easier to achieve than not acheive as we would otherwise have to
replicate a lot of functionality that already exists in system tools. Ultimately, we&rsquo;re better off
using the work already done than trying to create our own system that does the same thing.</p>

<p>The only exception to this goal is supporting LTO (Link Time Optimisation). However, it seems that
all other implementations of LTO share the same limitation, so it does not seem unreasonable to
share this limitation for Rust.</p>

<h2>Common Cases</h2>

<p>The phrase &ldquo;Common Case&rdquo; has a lot of baggage, as everybody has a different idea of what a &ldquo;common&rdquo;
case is. However, for the purposes of this discussion, I shall define &ldquo;common&rdquo; to be compiling
either a host platform executable or a host platform library. This should be broad enough to cover
most cases without too much controversy.</p>

<p>With that in mind, there are 4 types of files I have identified as common-case outputs</p>

<ul>
<li>Executable</li>
<li>Dynamic Library</li>
<li>Static Library</li>
<li>LTO Library</li>
</ul>


<p>In order to make linking behaviour the same across platforms, we need to specify what behaviour we
want in order to see what we might need to augment on what platforms to allow for the behaviour we
want. The behaviour, apart from standard linking behaviour, is as follows:</p>

<ul>
<li>Linking to a library does not add any extra explicit dependencies to the initial project. This is
the same for both dynamic and static libraries.</li>
<li>The default priority for linking libraries is LTO libraries then static libraries followed
finally by dynamic libraries. This can be controlled on an overall-level and a per-library level.
It should be possible to force a dynamic link even if a LTO library is available and it should be
possible to prioritise a dynamic library over a static one.</li>
<li>The path searched for libraries should start with command-line passed search paths, followed by
a rust-specific linker path environment variable (e.g. <code>RUST_LIBRARY_PATH</code>), finally followed by
the system linker path.</li>
<li>The default behaviour for linking to a static library should <em>not</em> cause the static library to
become a dependency. This could be overridden in cases where you know that the dependencies will
always be available. (For example, the core libraries)</li>
<li>The same rules should apply when linking in a non-rust library.</li>
</ul>


<p>Again, this should cover most common cases and is already similar to the existing model. The first
two points are related. The way libraries are chosen is based on the first match found by searching
the paths, meaning that a dynamic library that is on a path earlier than a static library will be
linked instead of the static library.</p>

<p>This behaviour is intended to be separate from any platform-specific behaviour, the intention being
to provide a consistent base that users can rely on. Ideally we should be able to &ldquo;fill in the
gaps&rdquo; where platform behaviour deviates from the described behaviour.</p>

<h2>Uncommon cases and People doing weird things</h2>

<p>These are cases that should be kept in mind, even though they might be more off of the radar than
most people will encounter.</p>

<ul>
<li>Cross-Compilation. Not particular out-there, but uncommon nonetheless. Ultimately, this should
just require an extra flag to the compiler to compile to a different target. There may be
interaction with linking to incompatible libraries though, so that needs to be kept in mind.</li>
<li>Non-standard linking process. This is doing stuff like using your own linker scripts to get finer
control over the linker. The most obvious example here I can think of are kernels, where custom
linker scripts are required to put symbols at known addresses.</li>
<li>Linking from a non-Rust environment. This is linking to a library from another natively-compiled
language, like C. While there are things to be aware of outside of just linking, we should try to
avoid making the process too difficult.</li>
<li>Linking in non-Rust object files. This is important for doing things like integrating assembly
files into the object.</li>
</ul>


<p>To some extent, merely providing tools to inspect rust crates and maintaining the existing
object-only output should help with most of these. Though handling many of them in the compiler
would be nice.</p>

<h2>Conclusion</h2>

<p>Supporting the uncommon cases is what I am most interested in here. Currently, Rust can be quite
difficult to use outside of the standard environment. I am aware though that the uncommon cases are
uncommon for a reason, and therefore have prioritised what <em>I</em> think the common cases are. I am
also undecided on the priority for the different library types, what is important is that
a priority exists, not what the priority actually is.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Paying technical debt in rustc]]></title>
    <link href="http://aatch.github.io/blog/2013/06/19/paying-technical-debt-in-rustc/"/>
    <updated>2013-06-19T14:00:00+12:00</updated>
    <id>http://aatch.github.io/blog/2013/06/19/paying-technical-debt-in-rustc</id>
    <content type="html"><![CDATA[<p>With the 0.7 release of Rust almost upon us, it seems like a good time talk about some of the
technical debt that has accrued in the compiler, the issues in the codegen module and plan of
action to try and start paying off some of that debt.</p>

<!-- more -->


<p>The rust compiler has two kinds of technical debt. The first is the normal kind of technical debt
where you say &ldquo;screw it, I&rsquo;ll do it properly later&rdquo;. This debt is common everywhere and generally
comes from a need to get it working quickly, or fixing a problem that is blocking the solution you
are actually working on. There isn&rsquo;t actually too much of this kind of debt in the rust code base.</p>

<p>The other kind of debt you find in the compiler is rather more unique. The Rust compiler is,
itself, written in Rust. This opens up whole new type of technical debt: not doing it properly
because it wasn&rsquo;t possible at the time. Whether this means having to work around a bug in order to
fix that bug, or having old patterns obsoleted by new features, this type of debt is much more
insidious. At the time of writing, the author of such code isn&rsquo;t aware of the debt he is creating.
He can&rsquo;t be. This means you end up with a proliferation of bad patterns and inconsistent code as
the langauge evolves and the codebase struggles to keep up.</p>

<h2>The debt in <code>trans</code></h2>

<p>For the uninitated, the codegen module in <code>librustc</code> lives in <code>middle::trans</code>, which I will refer
to as <code>trans</code> from here on in.</p>

<p><code>trans</code> is where the compiler stops being a very specific static analyzer and starts being an
actual compiler. Turning abstract representation into LLVM IR. This means that it is one of the
most critical parts of the compiler from a performance standpoint. Some of the major issues with
Rust are related to the output from this stage. One major issue is that our final IR is
significantly larger than it should be. However, I&rsquo;m not here to talk about what is wrong with the
functionality in <code>trans</code>.</p>

<p>Instead, I&rsquo;m going to talk about how the accrued technical debt in <code>trans</code> stops, or at least
hinders, fixing those issues.</p>

<h3>Incomplete, bad and non-existant abstractions</h3>

<p>An incredible number of functions in <code>trans</code> take a <code>TypeRef</code> or <code>ValueRef</code> directly. These are
LLVM objects and are completely opaque. Furthermore, we often end up needing information that
neither of these objects are capable of encoding, but don&rsquo;t have easily available. Often the code
uses suspicious work arounds or regenerates the required information. This is silly because we have
the full information from the type checker available to us.</p>

<p>Some abstractions do exist, like <code>Datum</code>, but the lack of full integration into the rest of the
module means that their use is limited. Even <code>Datum</code> is incomplete however, omitting useful
information that would allow us to make better decisions in code. This means the abstractions we do
have are bogged down trying to make sure they work with the rest of the code. It could be called
leaky, but this more like deliberately punching a hole in your roof so the birds can still get in.</p>

<p>Some of the biggest offenders are where we have only partial abstraction. Take for example
functions. We have a <code>fn_ctxt</code> object for &ldquo;abstracting&rdquo; functions, but you already need an LLVM
<code>ValueRef</code> for the function itself before you can create one. This means that <em>creating</em>
a function, first starts with <em>declaring</em> it with a completely unrelated function, then you can
start building it&rsquo;s body. Oh, and by the way, declaring a function consists of calling one three
functions: <code>register_fn</code>, <code>register_fn_full</code> or <code>register_fn_fuller</code>. If you find yourself writing
a function with the word &ldquo;fuller&rdquo; at the end, I suggest you take a step back and think about the
life choices that led you here.</p>

<p>This often means that a simple operation will trace through dozens of functions in several files.
A simple wrong turn can lead you down a path that might not <em>ever</em> be used any more, wasting time.</p>

<h3>Bad patterns</h3>

<p>A common pattern in old Rust code is to do something like this:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='rust'><span class='line'><span class="n">struct</span> <span class="n">Foo_</span> <span class="p">{</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">pre</span><span class="o">&gt;&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="n">num</span><span class="o">:</span> <span class="k">int</span>
</span><span class='line'><span class="o">&lt;/</span><span class="n">code</span><span class="o">&gt;&lt;/</span><span class="n">pre</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="p">}</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">pub</span> <span class="k">type</span> <span class="n">Foo</span> <span class="o">=</span> <span class="o">@</span><span class="k">mut</span> <span class="n">Foo_</span><span class="p">;</span>
</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>This code predates the powerful region system we have now, and (presumably) served to make life
easier, since you would otherwise need to pass around a <code>@mut Foo</code> explicitly. Times have changed
though, and this pattern only makes it harder to fix things. Patrick Walton recently informed me
that 7% of the time spent during compilation is on ref-count bumping. This is because a function
like this:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='rust'><span class='line'><span class="k">fn</span> <span class="n">bar</span><span class="p">(</span><span class="n">a</span><span class="o">:</span> <span class="n">Foo</span><span class="p">)</span> <span class="o">&amp;</span><span class="n">ndash</span><span class="p">;</span><span class="o">&gt;</span> <span class="k">int</span> <span class="p">{</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">pre</span><span class="o">&gt;&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="n">a</span><span class="p">.</span><span class="n">num</span>
</span><span class='line'><span class="o">&lt;/</span><span class="n">code</span><span class="o">&gt;&lt;/</span><span class="n">pre</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>does a ref-count increase for <code>Foo</code> when entering the function, then later drops that count when
the function ends. In the simple example above, that might be optimized out, but more often than
not there is no way for the optimizer to know that it can do that, so the useless ref-count bumps
stay.</p>

<p>Unfortunately, these patterns lead to more bad patterns, and then more bad patterns. A good example
is a statistic that relies on scoped destruction to construct nested contexts that get popped off
as the context ends. The current context is stored in a structure that gets passed around to most
functions, and the object that does the &ldquo;popping&rdquo; on destruction maintains a reference to that
context. This means that the object needs a mutable reference to that structure that lasts for the
entire function. This conflicts with the borrowing rules that prevent you from having multiple,
mutable borrows (or even a mutable and an immutable one at the same time). Thus the solution is
that you still need the <code>@mut</code> everywhere, even though that feature is literally the only thing
that needs it.</p>

<p>Similarly, there are plenty of other cases that only work because they rely on the nature of <code>@</code> or
<code>@mut</code> that could easily work another way with modern Rust.</p>

<h3>Poor naming and documentation</h3>

<p>Some things in <code>trans</code> are just badly named. Either they are misleading or opaque. This is because
of the &ldquo;I know what they mean&rdquo; effect. Some are somewhat benign, with the proliferation of <code>T_*</code>
functions being a little opaque, but are easy to figure out (LLVM TypeRef construction). Others are
maddening, like the <code>get_landing_pad</code> function in <code>base</code> that actually constructs the landing pad.
Other cases where there are dead arguments, or redundant arguments, or arguments are exist to
service one very specific case that infect every call to that function (or produce the crazy
<code>register_fn*</code> chain).</p>

<p>The poor documentation is more to do with the poor abstractions and naming than an absence of
comments. Often you&rsquo;ll need to trace the life of a particular piece of data, but be unable to find
where it comes into being. This is because some poorly-named function constructs it as a side
effect and it is later pulled out of some cryptic map by a line of code that <em>just knows</em> that
it&rsquo;ll be there. Unfortunately, more often than not, the reason you are trying to trace this
particular object is because some other line of code that <em>just knows</em> about the existence of that
object, is wrong.</p>

<h3>Death by a thousand paper cuts</h3>

<p>There are too many minor issues with the code to list them all. Individually they aren&rsquo;t
a significant problem, but they conspire to make it just a little <em>too</em> difficult to fix something.
Especially when that&rsquo;s not why you were there. Many of these issues could be fixed at the same time
as others, but the hassle it causes makes contributors (who often are volunteers working for
<em>free</em>) avoid it because they are trying to fix something else.</p>

<h2>How to fix it</h2>

<p>This is the sticking point, high-level ideas are nice, but I&rsquo;m not well-versed enough the code to
know exactly what we need in terms of new abstractions and refactorings. It&rsquo;s obvious that we need
to unify the function handling code, but what a function abstraction needs to look like is beyond
me.</p>

<p>I do know that something needs to be done, and I&rsquo;ve tried, but I&rsquo;ve hit the limit of my
understanding of the existing code and basic refactorings aren&rsquo;t going to help much anymore. In an
ideal world, we would just throw out <code>trans</code> and start again. Maybe salvaging what&rsquo;s there in order
to save time. Unfortunately, this is the real world and we can&rsquo;t stop all work on the compiler
while a module is being re-written. So the existing code needs to be moved, re-written, refactored
and improved, piece by piece until we get something that we don&rsquo;t have to ward newbies away from in
case they pick up bad habits.</p>
]]></content>
  </entry>
  
</feed>
